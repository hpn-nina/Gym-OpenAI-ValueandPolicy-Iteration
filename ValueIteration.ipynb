{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice:\r\n",
        "0,1,2,3: Left, down, right, up\r\n",
        "Reward: 0 for every step or falling in the hole, 1 for reaching the goal\r\n",
        "env return done if have taken 100 steps\r\n",
        "\r\n",
        "The environment’s step function returns exactly what we need. In fact, step returns four values. These are:\r\n",
        "\r\n",
        "    observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\r\n",
        "\r\n",
        "    reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\r\n",
        "\r\n",
        "    done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\r\n",
        "\r\n",
        "    info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\r\n",
        "\r\n",
        "    env.step(env.action_space.sample()) # take a random action\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2EfdvH5hSDXm"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iRO2o9axS4bS"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.render()\r\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(env.P[1][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-llzIO6xU9bm"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, max_iters, gamma):\n",
        "    v_values = np.zeros(env.observation_space.n) #Create an matrix with the same structure as the game\n",
        "    iterations = 0\n",
        "    for i in range(max_iters):\n",
        "        iterations +=1\n",
        "        prev_v_values = np.copy(v_values) #Prev_v_value contains the previous value \n",
        "\n",
        "        # Compute the value for state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            # Compute the q-value for each action\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Select the best action\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            iterations = i\n",
        "            break\n",
        "    \n",
        "    return v_values, iterations\n",
        "\n",
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int64)\n",
        "\n",
        "    # Compute the best action for each state in the game\n",
        "    # Compute q-value for each (state-action) pair in the game\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # Compute q_value for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # Select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cca5B02dZAl2"
      },
      "outputs": [],
      "source": [
        "def play(env):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    start = time.time()\n",
        "    v_values, iterations = value_iteration(env,max_iters=1000, gamma=0.9)\n",
        "    policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward, steps, end-start, iterations\n",
        "\n",
        "def play_multiple_times(env, itera, max_episodes):\n",
        "    with open('./FrozenLake-v0/Value-FrozenLake-v0-'+ str(itera)+'.txt',\"w+\") as writer:\n",
        "        success = 0\n",
        "        writer.write('Episode,Steps,Time,Converged at Iterations\\n')\n",
        "\n",
        "        for i in range(max_episodes):\n",
        "            reward, steps, timeTakes, iterations = play(env)\n",
        "            writer.write(str(i) + ',' + str(steps) + ',' + str(timeTakes) + ',' + str(iterations) + '\\n')\n",
        "\n",
        "            if reward > 0:\n",
        "                success += 1\n",
        "        \n",
        "        return success\n",
        "\n",
        "with open('./Success/Success-Value-FrozenLake-v0.txt','w+') as writer:\n",
        "    writer.write('Success\\n')\n",
        "    for i in range(50):\n",
        "        env = gym.make(\"FrozenLake-v0\")\n",
        "        success = play_multiple_times(env,i,max_episodes=1000)\n",
        "        writer.write(str(success)+'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play(env):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    start = time.time()\n",
        "    v_values, iterations = value_iteration(env,max_iters=1000, gamma=0.9)\n",
        "    policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward, steps, end-start, iterations\n",
        "\n",
        "\n",
        "def play_multiple_times(env, itera, max_episodes):\n",
        "    with open('./FrozenLake8x8-v0/Value-FrozenLake8x8-v0-'+ str(itera)+'.txt',\"w+\") as writer:\n",
        "        success = 0\n",
        "        writer.write('Episode,Steps,Time,Converged at Iterations\\n')\n",
        "\n",
        "        for i in range(max_episodes):\n",
        "            reward, steps, timeTakes, iterations = play(env)\n",
        "            writer.write(str(i) + ',' + str(steps) + ',' + str(timeTakes) + ',' + str(iterations) + '\\n')\n",
        "\n",
        "            if reward > 0:\n",
        "                success += 1\n",
        "        \n",
        "        return success\n",
        "\n",
        "with open('./Success/Success-Value-FrozenLake8x8-v0.txt','w+') as writer:\n",
        "    writer.write('Success\\n')\n",
        "    for i in range(50):\n",
        "        env = gym.make(\"FrozenLake8x8-v0\")\n",
        "        success = play_multiple_times(env,i,max_episodes=1000)\n",
        "        writer.write(str(success)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-nrMm7uyZ5we"
      },
      "outputs": [],
      "source": [
        "def playTaxi(env):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    start = time.time()\n",
        "    v_values, iterations = value_iteration(env,max_iters=1000, gamma=0.9)\n",
        "    policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward, steps, end-start, iterations\n",
        "\n",
        "def play_multiple_times_Taxi(env, itera, max_episodes):\n",
        "    with open('./Taxi-v3/Value-Taxi-v3-'+str(itera)+'.txt',\"w+\") as writer:\n",
        "        success = 0\n",
        "        writer.write('Episode,Steps,Time,Converged at Iterations\\n')\n",
        "\n",
        "        avg_rwd=0\n",
        "        for i in range(max_episodes):\n",
        "            reward, steps, timeTakes, iterations = playTaxi(env)\n",
        "            writer.write(str(i) + ',' + str(steps) + ',' + str(timeTakes) + ',' + str(iterations) + '\\n')\n",
        "\n",
        "            avg_rwd += reward\n",
        "        return avg_rwd/max_episodes\n",
        "\n",
        "with open('./Success/Success-Value-Taxi-v3.txt','w+') as writer:\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    writer.write('Success\\n')\n",
        "    for i in range(50):\n",
        "        writer.write(str(play_multiple_times_Taxi(env, i,max_episodes=100))+'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ValueIteration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python394jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
      "display_name": "Python 3.9.4 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}